{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom timeit import default_timer as timer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import Imputer\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom __future__ import division\nimport os\nimport gc, sys\ngc.enable()\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ab98d02c6b015df209a3cd84863a3cf438eeb9c"},"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    #start_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bbb861d6163e392ad3059ee8751cedf86fcda19"},"cell_type":"code","source":"def pre_process(train, is_train=True):\n    if is_train:\n        train=train[train['maxPlace']>1] ##Remove games which have only one player\n        y=train.groupby(['matchId','groupId'])['winPlacePerc'].agg('mean')\n        train=train.drop(['winPlacePerc'], axis=1)\n    train['totalDistance']=train['swimDistance']+train['walkDistance']+train['rideDistance']\n    train['headshotrate'] = train['kills']/train['headshotKills']\n    train['killStreakrate'] = train['killStreaks']/train['kills']\n    train['healthitems'] = train['heals'] + train['boosts']\n    #train=train.drop(['heals','boosts'], axis=1)\n    train['totalDistance'] = train['rideDistance'] + train[\"walkDistance\"] + train[\"swimDistance\"]\n    train['killPlace_over_maxPlace'] = train['killPlace'] / train['maxPlace']\n    train['headshotKills_over_kills'] = train['headshotKills'] / train['kills']\n    train['distance_over_weapons'] = train['totalDistance'] / train['weaponsAcquired']\n    train['walkDistance_over_heals'] = train['walkDistance'] / train['heals']\n    train['walkDistance_over_kills'] = train['walkDistance'] / train['kills']\n    train['killsPerWalkDistance'] = train['kills'] / train['walkDistance']\n    train[\"skill\"] = train[\"headshotKills\"] + train[\"roadKills\"]\n    train[train == np.Inf] = np.NaN\n    train[train == np.NINF] = np.NaN\n    print(\"Removing Na's From DF\")\n    train.fillna(0, inplace=True)\n    #train=train.drop(['swimDistance', 'walkDistance','rideDistance'], axis=1)\n    train['teamSize']=train.groupby('groupId')['groupId'].transform('count')\n    features=list(train.columns)\n    features.remove(\"Id\")\n    features.remove(\"numGroups\")\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n    features.remove(\"matchType\")\n    train_out = train.groupby(['matchId','groupId'])[features].agg({'avg':'mean','max': 'max', 'min': 'min','sum':'sum'})\n    #'dev':'std'\n    train_out.columns = [\"_\".join(x) for x in train_out.columns.ravel()]\n    #train_out=train_out.drop(['dev_teamSize'], axis=1)\n    train_out=train_out.replace([np.inf, -np.inf], 0)\n    train_out=train_out.drop(['max_teamSize', 'min_teamSize','sum_teamSize'], axis=1)\n    features=list(train_out.columns)\n    train_rank = train_out.groupby('matchId')[features].rank(pct=True, na_option= 'top')\n    train_final = train_out.reset_index()[['matchId','groupId']]\n    train_final = train_final.merge(train_out.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    train_final = train_final.merge(train_rank, suffixes=[\"\",\"_rank\"], how='left', on=['matchId', 'groupId'])\n    train_final['matchSize']=train_final.groupby('matchId')['matchId'].transform('count')\n    if is_train:\n        train_final=train_final.drop(['matchId', 'groupId'], axis=1)\n    train_final=reduce_mem_usage(train_final)\n    del train_rank, train_out, train\n    gc.collect()\n    if is_train:\n        return train_final, y\n    return train_final","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_main=pd.read_csv(\"../input/train_V2.csv\")\ntest=pd.read_csv(\"../input/test_V2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ceda1151f86e19736e432ff87e4daf0cbf580e37"},"cell_type":"code","source":"train_main=reduce_mem_usage(train_main)\ntest=reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63cf4337d6332ced876b92daea1e80fe7ea8ff4d"},"cell_type":"code","source":"##the validation and training splits have to be made based on the matchId's as taking random points may mislead the classifier\ntrain_match, val_match = model_selection.train_test_split(train_main.matchId.unique())\nX_train=train_main.loc[train_main['matchId'].isin(train_match)]\nX_val=train_main.loc[train_main['matchId'].isin(val_match)]\ndel train_main\nprint('Training data shape : ', X_train.shape)\nprint('Validation data shape: ', X_val.shape)\nprint('Test data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b369b8dcb5dcc423492ea34bcc99ed76fda28122"},"cell_type":"code","source":"X_train, y_train=pre_process(X_train, is_train=True)\nX_val, y_val=pre_process(X_val, is_train=True)\ntest=pre_process(test, is_train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64b92a36548252bd2520ec49be89fd723a421214"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e2ce20aae93cdd784097ac2fd84576468013b8f"},"cell_type":"code","source":"def simple_regression(X, y):\n    lr=LinearRegression()\n    lr.fit(X, y)\n    print(\"Prediction error: \", mean_absolute_error(y, lr.predict(X)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66af1f2d92ce067cd06c61709db13a97b69de457"},"cell_type":"markdown","source":"There are different sets of matches and each match has a number of groupIds. The target values can be obtained by averaging the finish percentages of a specific group as all the members of the group will have the same finish percentage. "},{"metadata":{"trusted":true,"_uuid":"3f5b2ffe8ef7e2e297e3674ad8b59b70be51e54b"},"cell_type":"code","source":"\"\"\"\nxgbreg = xgb.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', booster='gbtree', n_jobs=-1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=9, seed=9, missing=None, importance_type='gain')\nxgbreg.fit(train_final ,y,verbose = 50)\nprint(\"Prediction error: \", mean_absolute_error(y, xgbreg.predict(train_final)))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d3fab470b580b3b547e6dd87fc03363993f28d1"},"cell_type":"code","source":"params = {\"objective\" : \"regression\", \"metric\" : \"mae\", 'n_estimators':20000, 'early_stopping_rounds':200,\n              \"num_leaves\" : 31, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.7,\n               \"bagging_seed\" : 0, \"num_threads\" : 4,\"colsample_bytree\" : 0.7}\nlgtrain = lgb.Dataset(X_train, label=y_train)\nlgval = lgb.Dataset(X_val, label=y_val)\nmodel = lgb.train(params, lgtrain, valid_sets=[lgtrain, lgval], early_stopping_rounds=200, verbose_eval=1000)\npred_train_y = model.predict(X_train, num_iteration=model.best_iteration)\npred_val_y = model.predict(X_val, num_iteration=model.best_iteration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efec34c4e979b78d2352588f644af697fc5eb17e"},"cell_type":"code","source":"print(\"Prediction error: \", mean_absolute_error(y_val, pred_val_y ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"058f7ac2b549848e7f5759f465b8c1772c4ab4da"},"cell_type":"code","source":"pred_test_y=model.predict(test.drop(['matchId','groupId'], axis=1), num_iteration=model.best_iteration)\npred_y=pd.DataFrame({'winPlacePerc':pred_test_y})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83d937fe2fb434a48c4f02116fde5a77c5b9e75f"},"cell_type":"code","source":"test=pd.read_csv(\"../input/test_V2.csv\")\nfinal_out=test[['Id', 'groupId']]\ntest=pre_process(test, is_train=False)\nvalues=zip(list(test['groupId'].values), list(pred_test_y))\ndf = pd.DataFrame(data = list(values), columns=['groupId', 'winPlacePerc'])\nfinal_out=final_out.merge(df, on='groupId', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5977dfabaa4ebbc7bc0775abcef4830fff81544"},"cell_type":"code","source":"final_out=final_out.drop(['groupId'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de6123a8865cfca3dcb32873896f7c16656e24be"},"cell_type":"code","source":"submission = final_out\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37f7314be36716a5361c5419e3acf18ba0a92c93"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}